import streamlit as st
import time
from scipy.integrate import solve_ivp
from pypdf import PdfReader
from docx import Document

# --- CONFIGURATION GEMINI-LIKE ---
st.set_page_config(page_title="VTM Intelligence", page_icon="‚öõÔ∏è", layout="centered")

st.markdown("""
    <style>
    .stApp { background-color: #131314; color: #e3e3e3; }
    .stChatInputContainer { background-color: #1e1f20; border-radius: 28px; border: 1px solid #444746; }
    .stChatMessage { background-color: transparent !important; }
    /* Masquer les √©l√©ments techniques */
    .stStatus { border: none !important; background: transparent !important; }
    </style>
""", unsafe_allow_html=True)

# --- MOTEUR DE RAISONNEMENT INVISIBLE ---
class VTMBrain:
    def __init__(self, matrix_text):
        self.matrix = matrix_text

    def internal_reasoning(self, query):
        """ Calcule la stabilit√© en arri√®re-plan (sans affichage) """
        phi_c = len(query) / 10.0
        # Syst√®me dynamique TTU-MC3
        def flow(t, y):
            return [-0.6*y[0] + 1.2*y[1], -0.7*y[1] + 0.8*y[0]*y[2], 0.5*y[1]**2 - 0.3*y[2]]
        # On r√©sout pour valider la coh√©rence de la pens√©e
        sol = solve_ivp(flow, [0, 5], [1.0, phi_c / 9.0, 0.1])
        return sol.y[0, -1] > 0.5  # Retourne si la pens√©e est stabilis√©e

    def generate_response(self, query):
        """ Synth√©tise une r√©ponse claire bas√©e sur le savoir local ou global """
        if self.matrix:
            # Recherche de r√©sonance dans tes th√®ses
            segments = [s for s in self.matrix.split('.') if any(w in s.lower() for w in query.lower().split())]
            if segments:
                return f"{segments[0].strip()}. Cela s'inscrit dans la dynamique de stabilit√© structurelle de vos travaux."
        
        # R√©ponse autonome si la matrice est vide ou ne contient pas la r√©ponse
        return "L'intelligence, dans ce contexte, est la capacit√© √† transformer le flux d'informations du monde en une structure coh√©rente et stable. C'est un √©quilibre permanent entre la m√©moire acquise et la dissipation n√©cessaire au renouveau."

# --- INTERFACE DE CHAT ---
if "messages" not in st.session_state:
    st.session_state.messages = []
if "vault" not in st.session_state:
    st.session_state.vault = ""

# Sidebar discr√®te pour charger la connaissance
with st.sidebar:
    st.title("üìÇ Matrice")
    uploaded = st.file_uploader("Fichiers doctoraux", accept_multiple_files=True)
    if uploaded:
        text = ""
        for f in uploaded:
            if f.name.endswith('.pdf'):
                pdf = PdfReader(f); text += " ".join([p.extract_text() for p in pdf.pages])
            elif f.name.endswith('.docx'):
                doc = Document(f); text += " ".join([p.text for p in doc.paragraphs])
        st.session_state.vault = text
        st.success("Connaissance int√©gr√©e.")

st.title("‚öõÔ∏è VTM Intelligence")

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

if prompt := st.chat_input("Posez votre question..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        response_placeholder = st.empty()
        
        # L'IA "r√©fl√©chit" (Calcul VTM invisible)
        with st.status("Analyse en cours...", expanded=False) as status:
            brain = VTMBrain(st.session_state.vault)
            is_stable = brain.internal_reasoning(prompt)
            time.sleep(0.8)
            status.update(label="R√©flexion termin√©e", state="complete")

        # R√©sultat de la r√©flexion
        answer = brain.generate_response(prompt)

        # Animation d'√©criture fluide (Style Gemini)
        full_text = ""
        for chunk in answer.split():
            full_text += chunk + " "
            response_placeholder.markdown(full_text + "‚ñå")
            time.sleep(0.04)
        response_placeholder.markdown(full_text)
        
    st.session_state.messages.append({"role": "assistant", "content": full_text})

import streamlit as st
import numpy as np
import matplotlib.pyplot as plt
import yaml
import hashlib
import time

# Imports locaux
from ttu_kernel import TTU_Master_Kernel
from ttu_bridge import TTU_LLM_Bridge
from utils import prompt_to_signal, plot_attractor, plot_time_series

# Configuration de la page
st.set_page_config(page_title="TTU-MC3 Chatbot", page_icon="üåÄ", layout="wide")

# Titre et description
st.title("üåÄ Chatbot TTU-MC3 : IA g√©n√©rative autonome")
st.markdown("""
Ce chatbot utilise un moteur TTU-MC3 en arri√®re-plan pour influencer la g√©n√©ration de texte.
Chaque question est transform√©e en signal qui excite le syst√®me dynamique.
L'attracteur obtenu produit des param√®tres (temp√©rature, top_p) et une "substance" qui guident le mod√®le local GPT-2.
""")

# Barre lat√©rale : param√®tres du kernel et options
with st.sidebar:
    st.header("‚öôÔ∏è Param√®tres TTU")
    alpha = st.number_input("Œ± (Amortissement m√©moire)", value=0.0001, format="%.5f", step=0.0001)
    beta = st.number_input("Œ≤ (Couplage Dissipation-M√©moire)", value=0.5, format="%.2f", step=0.1)
    gamma = st.number_input("Œ≥ (Gain de Coh√©rence)", value=1.2, format="%.2f", step=0.1)
    lambda_ = st.number_input("Œª (Couplage non-lin√©aire)", value=4.0, format="%.2f", step=0.1)
    mu = st.number_input("Œº (Friction cubique)", value=3.0, format="%.2f", step=0.1)

    st.subheader("√âtat initial")
    pm0 = st.number_input("Œ¶m (M√©moire)", value=15.0)
    pc0 = st.number_input("Œ¶c (Coh√©rence)", value=0.5)
    pd0 = st.number_input("Œ¶d (Dissipation)", value=0.2)

    st.subheader("Simulation")
    t_max = st.number_input("Dur√©e d'int√©gration (secondes simul√©es)", value=10.0, min_value=1.0, max_value=50.0, step=1.0)
    n_points = st.number_input("Nombre de points", value=2000, min_value=500, max_value=10000, step=500)
    method = st.selectbox("M√©thode d'int√©gration", ["BDF", "RK45", "LSODA"])

    st.subheader("Mod√®le de langage")
    model_name = st.selectbox("Mod√®le", ["gpt2", "distilgpt2", "microsoft/DialoGPT-small"])
    use_web_noise = st.checkbox("Ajouter du bruit 'web' au signal", value=True)

    st.subheader("Affichage")
    show_attractor = st.checkbox("Afficher l'attracteur apr√®s chaque r√©ponse", value=False)
    show_params = st.checkbox("Afficher les param√®tres s√©mantiques", value=True)

# Initialisation de l'historique de conversation
if "messages" not in st.session_state:
    st.session_state.messages = []

# Cache du mod√®le LLM (charg√© une seule fois)
@st.cache_resource
def load_llm(model_name):
    from transformers import pipeline, set_seed
    set_seed(42)
    return pipeline('text-generation', model=model_name)

# Fonction de g√©n√©ration de r√©ponse
def generate_response(prompt, history, params, initial_state, t_max, n_points, method, model_name, use_web_noise):
    # Cr√©er le kernel avec les param√®tres actuels
    kernel = TTU_Master_Kernel(params, initial_state)

    # Construire le signal √† partir du prompt + √©ventuel bruit
    def signal_func(t):
        sig = prompt_to_signal(t, prompt, freq_base=1.0)
        if use_web_noise:
            sig += 0.05 * np.random.normal()
        return sig

    # Int√©gration
    t_span = (0, t_max)
    t_eval = np.linspace(0, t_max, n_points)
    sol = kernel.run_sequence(t_span, t_eval, signal_func=signal_func, method=method)

    # Extraction de la substance (√©chantillonnage adapt√©)
    sampling_rate = max(1, n_points // 50)  # environ 50 √©chantillons
    substance = kernel.extract_substance(sampling_rate=sampling_rate)

    # Param√®tres s√©mantiques
    bridge = TTU_LLM_Bridge(kernel)
    semantic = bridge.extract_semantic_vector()

    # Construction du prompt pour le LLM
    # On inclut les derniers √©changes (jusqu'√† 6 messages) et la substance
    context = "\n".join([f"{m['role']}: {m['content']}" for m in history[-6:]])
    llm_prompt = f"{context}\n{substance[:100]}\nUser: {prompt}\nAssistant:"

    # G√©n√©ration avec le mod√®le
    try:
        generator = load_llm(model_name)
        results = generator(
            llm_prompt,
            max_length=150,
            temperature=semantic['temperature'],
            top_p=semantic['top_p'],
            do_sample=True,
            num_return_sequences=1,
            pad_token_id=50256  # pour gpt2
        )
        reply = results[0]['generated_text'].replace(llm_prompt, "").strip()
        if not reply:
            reply = "..."
    except Exception as e:
        reply = f"[Erreur du mod√®le: {e}]"

    return reply, sol, substance, semantic

# Interface de chat
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# Zone de saisie
if prompt := st.chat_input("Posez votre question..."):
    # Ajouter le message utilisateur
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # G√©n√©ration de la r√©ponse
    with st.chat_message("assistant"):
        with st.spinner("Le cristal TTU oscille..."):
            params = {
                'alpha': alpha,
                'beta': beta,
                'gamma': gamma,
                'lambda_': lambda_,
                'mu': mu
            }
            initial_state = [pm0, pc0, pd0]
            reply, sol, substance, semantic = generate_response(
                prompt,
                st.session_state.messages,
                params,
                initial_state,
                t_max,
                n_points,
                method,
                model_name,
                use_web_noise
            )
            st.markdown(reply)

            # Affichage optionnel des infos
            if show_params:
                with st.expander("üîÆ Param√®tres s√©mantiques extraits"):
                    st.write(f"**Temp√©rature**: {semantic['temperature']:.4f}")
                    st.write(f"**Top_p**: {semantic['top_p']:.4f}")
                    st.write(f"**Substance (extrait)**: `{substance[:100]}`")

            if show_attractor and sol is not None:
                with st.expander("üåÄ Attracteur Œ¶c vs Œ¶d"):
                    pc, pd = kernel.get_attractor_data()
                    if pc is not None:
                        fig = plot_attractor(pc, pd, title="Attracteur pour cette question")
                        st.pyplot(fig)
                        plt.close(fig)

    # Ajouter la r√©ponse √† l'historique
    st.session_state.messages.append({"role": "assistant", "content": reply})

# Pied de page
st.markdown("---")
st.markdown("**TTU-MC3 - Scellement Isomorphique Certifi√©**")
import streamlit as st
import torch
import yaml
import os
from pathlib import Path
from ttu_model import TTULanguageModel
from utils import plot_trajectory_3d, search_wikipedia
import numpy as np

# Configuration de la page
st.set_page_config(page_title="TTU-MC¬≥ AI Avanc√©", layout="wide")
st.title("üß†üí¨ TTU-MC¬≥ AI - Chatbot √† raisonnement avanc√©")
st.markdown("Un assistant conversationnel bas√© sur **DialoGPT** + dynamique triadique dissipative + recherche de connaissances.")

# Chargement de la configuration
with open("config.yaml", "r") as f:
    config = yaml.safe_load(f)

# Chargement du mod√®le en cache (pour Streamlit Cloud)
@st.cache_resource
def load_model():
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = TTULanguageModel(
        base_model_name=config['model']['name'],
        hidden_dim=config['model']['hidden_dim'],
        dt=config['model']['dt']
    ).to(device)
    return model, device

model, device = load_model()

# √âtat de session
if "history" not in st.session_state:
    st.session_state.history = []               # messages √©chang√©s
    st.session_state.ttu_state = None            # √©tat interne TTU
    st.session_state.traj = []                   # trajectoire TTU
    st.session_state.knowledge_enabled = config['knowledge']['enabled']

# Barre lat√©rale
st.sidebar.header("‚öôÔ∏è Param√®tres de g√©n√©ration")
temperature = st.sidebar.slider("Temp√©rature", 0.1, 2.0, config['model']['temperature'], 0.1)
max_new_tokens = st.sidebar.slider("Max nouveaux tokens", 50, 500, 150, 10)
repetition_penalty = st.sidebar.slider("P√©nalit√© de r√©p√©tition", 1.0, 2.0, config['model']['repetition_penalty'], 0.1)
knowledge_enabled = st.sidebar.checkbox("Activer recherche Wikip√©dia", value=st.session_state.knowledge_enabled)
st.session_state.knowledge_enabled = knowledge_enabled

mode = st.sidebar.selectbox("Mode", ["Standard", "Dissipation active", "Silence dissipatif", "Exploration"])
if mode == "Dissipation active":
    st.sidebar.info("Cr√©ativit√© maximale")
elif mode == "Silence dissipatif":
    st.sidebar.info("Stabilit√©")
elif mode == "Exploration":
    st.sidebar.info("Exploration")

if st.sidebar.button("üóëÔ∏è Nouvelle conversation"):
    st.session_state.history = []
    st.session_state.ttu_state = None
    st.session_state.traj = []
    st.rerun()

# Interface principale : deux colonnes
col1, col2 = st.columns([2, 1])

with col1:
    st.header("üí¨ Conversation")

    # Afficher l'historique des messages
    for msg in st.session_state.history:
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    # Zone de saisie
    prompt = st.chat_input("Posez votre question...")
    if prompt:
        # Afficher le message utilisateur
        with st.chat_message("user"):
            st.markdown(prompt)
        st.session_state.history.append({"role": "user", "content": prompt})

        # Recherche de connaissances (si activ√©e)
        knowledge = None
        if knowledge_enabled:
            with st.spinner("Recherche de connaissances..."):
                knowledge = search_wikipedia(prompt, max_sentences=config['knowledge']['max_summary_sentences'])
            if knowledge:
                st.info(f"Contexte trouv√© : {knowledge}")

        # G√©n√©ration de la r√©ponse
        with st.chat_message("assistant"):
            with st.spinner("R√©flexion..."):
                response, new_state, traj = model.generate(
                    prompt,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    repetition_penalty=repetition_penalty,
                    ttu_state=st.session_state.ttu_state,
                    knowledge=knowledge
                )
                st.markdown(response)
                st.session_state.history.append({"role": "assistant", "content": response})
                st.session_state.ttu_state = new_state
                st.session_state.traj = traj

with col2:
    st.header("üìà Visualisation TTU")
    if st.session_state.traj:
        fig = plot_trajectory_3d(st.session_state.traj)
        st.plotly_chart(fig, use_container_width=True)
        last = st.session_state.traj[-1][0]
        st.metric("Coh√©rence (œï_C)", f"{last[0]:.3f}")
        st.metric("Dissipation (œï_D)", f"{last[1]:.3f}")
        st.metric("M√©moire (œï_M)", f"{last[2]:.3f}")
    else:
        st.info("Posez une question pour voir la trajectoire.")

    # Boutons d'exemples
    st.subheader("Exemples de questions")
    if st.button("Hypoth√®se de Riemann"):
        st.session_state.history.append({"role": "user", "content": "Explique l'hypoth√®se de Riemann"})
        st.rerun()
    if st.button("Th√©or√®me de Fermat"):
        st.session_state.history.append({"role": "user", "content": "Qu'est-ce que le dernier th√©or√®me de Fermat ?"})
        st.rerun()
    if st.button("Qu'est-ce que la beaut√© ?"):
        st.session_state.history.append({"role": "user", "content": "Qu'est-ce que la beaut√© selon les philosophes ?"})
        st.rerun()
    if st.button("Savoir ou croyances"):
        st.session_state.history.append({"role": "user", "content": "Savoir ou croyances. √âpilogue"})
        st.rerun()

# Pied de page
st.markdown("---")
st.markdown("**TTU-MC¬≥** ‚Äî Th√©orie Triadique Unifi√©e ‚Äî [Documentation](https://github.com/votre_nom/ttu-ai-advanced)")